{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# MCIS6273 Data Mining (Prof. Maull) / Fall 2020 / HW3\n", "\n", "**This assignment is worth up to 15 POINTS to your grade total if you complete it on time.**\n", "\n", "| Points <br/>Possible | Due Date | Time Commitment <br/>(estimated) |\n", "|:---------------:|:--------:|:---------------:|\n", "| 15 | Wednesday, November 18 @ Midnight | _up to_ 8 hours |\n", "\n", "\n", "* **GRADING:** Grading will be aligned with the completeness of the objectives.\n", "\n", "* **INDEPENDENT WORK:** Copying, cheating, plagiarism  and academic dishonesty _are not tolerated_ by University or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.\n", "\n", "## OBJECTIVES\n", "* Perform Bayesian text classification\n", "\n", "## WHAT TO TURN IN\n", "You are being encouraged to turn the assignment in using the provided\n", "Jupyter Notebook.  To do so, make a directory in your Lab environment called\n", "`homework/hwN`.   Put all of your files in that directory.  Then zip that directory,\n", "rename it with your name as the first part of the filename (e.g. `maull_hwN_files.zip`), then\n", "download it to your local machine, then upload the `.zip` to Blackboard.\n", "\n", "If you do not know how to do this, please ask, or visit one of the many tutorials out there\n", "on the basics of using zip in Linux.\n", "\n", "If you choose not to use the provided notebook, you will still need to turn in a\n", "`.ipynb` Jupyter Notebook and corresponding files according to the instructions in\n", "this homework.\n", "\n", "\n", "## ASSIGNMENT TASKS\n", "### (100%) Perform Bayesian text classification \n", "\n", "Text classification is an important application area of machine learning.  Indeed, the early\n", "advances in the field were in text and image processing.  We are now beneficiaries of the\n", "libraries that provide us the foundation for a variety of techniques to do sophisticated text\n", "analytics and process withuout much effort.\n", "\n", "With text classification, one goal we might like to accomplish is determine the original of\n", "particular text.  What once used to be the arena of computational linguists and computer\n", "scientists, is now growing in [computational digital humanities](https://jitp.commons.gc.cuny.edu/a-survey-of-digital-humanities-programs/),\n", "but is [not without issues](https://www.chronicle.com/article/the-digital-humanities-debacle/).  In this assignment\n", "we are going to use Bayesian techniques to process a corpus, or body of text, with the expressed\n", "goal of classifying it.  In fact, we're going to take multiple texts and generate a classifier\n", "that (with some work), will be able to distinguish between multiple authors.\n", "\n", "Laying out the intuition of the technique, let's abstractly think about the problem at hand.  An author\n", "usually writes in a particular style, uses specific words, chooses particular topics, and as\n", "such, these things contribute to a \"profile\" (loosely speaking) of the author's works.  More\n", "concretely, the probability that Albert Einstein used a word like \"thou\" is much lower than\n", "the probability that Shakespeare would use the same word.  Ultimately with enough examples of the\n", "writings of a particular author, the easier it would be to establish the probabilities of certain\n", "words, phrases and even punctuation usage.  While we are going to choose to classify authors, the\n", "same technique could be used to classify content themes, for example, classifying texts about\n", "_philosophy_ versus texts about _turtles_.\n", "\n", "Bayesian techniques are a mainstain, and the ease with which Bayes classifiers can be trained\n", "make it a technique that can be fast to implement and get results that are often very accurate.\n", "\n", "In the interest of time and resouces, we're going to develop a simple Bayesian text classifier to distinguish\n", "between the writings of three philosophers -- [Aristotle](https://plato.stanford.edu/entries/aristotle/),\n", "[Plato](https://plato.stanford.edu/entries/plato/) and [David Hume](https://plato.stanford.edu/entries/hume/).\n", "Under ordinary circumstances\n", "we would like to have as many documents from each of these authors as possible -- instead, we\n", "will train from 3 documents each and test against 3 more.  Again, if we were going to develop\n", "a much more robust classifier we would likely choose a much larger number of documents if they were\n", "possible.  Luckily, there are enough writings to have enough to work with in this abbreviated exploration.\n", "\n", "At the heart of document classification is the _model_ for document features.  One popular model is\n", "the TF-IDF or Term Frequency Inverse Document Frequency.  The intuition behind analyzing words in\n", "documents hinges on the following:\n", "\n", "* terms that are frequent _in documents_ are given higher importance than those that are infrequent,\n", "* terms that are frequent _across_ documents are not considered as important;\n", "\n", "that is _common_ words across an entire corpus are *discounted* while\n", "those that are _common_ within documents are *boosted*.  This is an effective way to differentiate since\n", "the intuition that the things that make your writing unique are amplified, while those that are not\n", "differentiators will count less.\n", "\n", "\n", "To realize the TF-IDF, we will need to break apart the two components TF (or **term frequency**) and\n", " IDF (**inverse document frequency**) and then conjoin them.\n", "\n", "**Term frequency (TF)** is a simple concept and is exactly as it says: the _counts_ of terms in a document.\n", "So for a term (word) $t$ and document $d$, the TF is just the number of occurences of $t$ in $d$,\n", "\n", "$$\\textrm{tf}(t,d) = \\big| t \\in d \\big|$$\n", "\n", "**Inverse document frequency (IDF)** provides a way to determine if a terms is rare or\n", "common given _all_ documents $D$, and is logarithmically scaled so rare terms avoid completely disappearing.  Thus,\n", "\n", "$$\n", "\\textrm{idf}(t,D) = \\frac{\\big| D \\big|}{ 1 + \\big| \\{t \\in d | d \\in D \\} \\big| }\n", "$$\n", "\n", "**TF-IDF** is thus: for a set of documents (corpus) $D$ and document $d \\in D$ and terms $t \\in d$,\n", "\n", "\n", "$$\n", "\\textrm{tfidf}(t,d,D)= \\textrm{tf}(t,d,D) \\times \\textrm{idf}(t,D)\n", "$$\n", "\n", "\n", "Luckily, `sklearn` implements TF-IDF for us in the [`sklearn.feature_extraction.text.TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=vectorizer#sklearn.feature_extraction.text.TfidfVectorizer)\n", "class.\n", "The underlying implementation uses the words as the feature matrix where the TF-IDF is computed over\n", "every document input to the [`vectorizer.fit_transform()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=vectorizer#sklearn.feature_extraction.text.TfidfVectorizer.transform)\n", "method.\n", "\n", "Now that we've implemented to the primary machinery of the method, let's bring back Bayesian.  Recall the\n", "Bayesian method:\n", "\n", "$$ \\Pr(C \\big| w_1, \\ldots, w_n) = \\Pr( C ) \\prod_i^n \\Pr(  w_i \\big| C ) $$\n", "\n", "where $C$ is the document class (Plato or class `A`, Hume or class `B` and Aristotle or class `C`) and $w_i$\n", "the words in the document.  Concretely, a document $D_i$ has some probability $P_i$ based on the\n", "occurrence of the words $w_i$ in that document, and that a classifier will decide the class $\\hat{C}$  of document\n", "$D_i$ by computing\n", "\n", "$$ \\hat{C} = \\mathrm{argmax}_C \\Pr( C ) \\prod_i^n \\Pr(  w_i \\big| C )$$\n", "\n", "by training the classifier on some labeled data.  Once trained the classifier can be tested and then used on\n", "unlabelled data to classify the author.  While this exercise is decidely oversimplified (we'd not really be all\n", "that interested in classifying the works of only 3 authors), you can extend this to other domains where\n", "perhaps you're not classifying authors, but styles, topics or document complexity.\n", "\n", "Completing the assignment will require you use the provided notebook and corresponding data files.\n", "This notebook can be found in `example_notebook.ipynb`.  Study it closely.\n", "\n", "&#167;  Using the notebook provided and corresponding files, you are to write a Python\n", "function to load the training data.  Use the dictionary map in the variable\n", "`training_map`.  Your function will take the files (in the order they appear in\n", "`training_map`) and pass the  data into the [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn-feature-extraction-text-tfidfvectorizer) vectorizer.  You\n", "will need to set the parameter to the constructor to `input='file'` and the\n", "`stop_words` to `'english'` (e.g. initialize the vectorizer to `TfidfVectorizer(input='file', stop_words='english')`.\n", "\n", "* **You will just need to show the new function and the initialization of the vectorizer\n", "in this step.**  This will be one or two cells at most.\n", "* You will use [`fit_transform()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.fit_transform) with the parameter being a list of the training files.\n", "\n", "\n", "&#167;  Now that you have a vectorizer which effectively builds the data structure to hold the\n", "TF-IDF of all the words which appear for each document, you can move to the training\n", "phase for the Bayesian classifier.  Look in the sample notebook for guidance. You will take as\n", "input the vectorizer output (the documents vectorized by TF-IDF) and the corresponding\n", "classes (in the order they\n", "appear in the original dictionary map) and pass that into the [`MultinomialNB.fit()`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB.fit) method.\n", "\n", "* **Show the initialization of your `MultinomialNB()` classifier and the application of the `fit()` method.**\n", "\n", "\n", "&#167;  Now that you have a vectorizer and a classifier trained, let's classify some documents. The vectorizer you\n", "created in the first step has a [`transform`]() method, which you can use\n", "to vectorize a new document and pass as test input to the classifier.  Once you have\n", "the vectorized version of the document, you can use the classifier's [`MultinomialNB.predict(test_data)`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB.predict)\n", "method, which will return the class label according to the classifier.\n", "\n", "In your notebook do the following:\n", "\n", "1. **Write a function to take as input a vectorized document and trained classifier and return\n", "the predicted label for the document.**  See the sample notebook for guidance.\n", "\n", "1. **Test on the files in the `data/philosopher_name/test` folders and show the output of your test.**\n", "You can wrap your function from the previous step in a loop\n", "to run through all data in the folder.  This will be short enough to be coded in a single Jupyter cell.\n", "\n", "To get the full points, please show your work.\n", "\n", "\n", "&#167;  You have now built your first document classifier!  Now answer the following questions:\n", "\n", "1. How many of the documents did your classifier correctly classify?\n", "\n", "\n", "&#167;  The classifier `predict` method only returns the label, but you can get the probabilities assigned to all\n", "classes using [`predict_proba()`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB.predict_proba).\n", "Please take a look at the example notebook to see how that is done.\n", "\n", "**Answer the following questions inside your notebook:**\n", "\n", "1. Make an observation about the class probabilities.  What did you notice?\n", "2. Provide some commentary on how the probabilities might be improved (you can provide you answer\n", "   as a thought exercise or if you have time, provide some example code).\n", "\n", "\n", "\n"]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python [default]", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.1"}, "toc": {"colors": {"hover_highlight": "#DAA520", "navigate_num": "#000000", "navigate_text": "#333333", "running_highlight": "#FF0000", "selected_highlight": "#FFD700", "sidebar_border": "#EEEEEE", "wrapper_background": "#FFFFFF"}, "moveMenuLeft": true, "nav_menu": {"height": "12px", "width": "252px"}, "navigate_menu": true, "number_sections": false, "sideBar": true, "threshold": "1", "toc_cell": false, "toc_section_display": "block", "toc_window_display": true, "widenNotebook": false}}, "nbformat": 4, "nbformat_minor": 0}